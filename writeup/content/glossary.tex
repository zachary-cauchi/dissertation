\newglossaryentry{G}
{
    name=\(\mathcal{G}\),
    description={Loop-free directed graph representing the network topology},
    sort=G
}
\newglossaryentry{V}
{
    name=\(V\),
    description={The set of nodes},
    sort=V
}
\newglossaryentry{E}
{
    name=\(E\),
    description={The set of links},
    sort=E
}

\newglossaryentry{token}
{
    name=token,
    description={A word, number, symbol, or other character forming part of a sentence or array of tokens},
    sort=token
}

\newglossaryentry{corpus}
{
    name=corpus,
    description={A file containing every tokenised sentence in a dataset / series of sentences},
    sort=corpus
}
\newglossaryentry{rnn_g}
{
    name={RNN},
    description={A Recurrent Neural Network (RNN) is a class of neural network structures that
            can use its own output as input, in a cyclic manner. This allows it to process input data where order of sequence is important in deriving output, or process data of variable length},
    sort=rnn
}
\newglossaryentry{logit}
{
    name={logit},
    description={A function which represents a probability measured from 0 to 1. Mathematically, a logit is represented as \(logit(p)=log(\frac{p}{1-p})\)},
    sort=logit
}
\newglossaryentry{softmax}
{
    name={softmax},
    description={A vector of \textit{n} possible choices --- represented as \textit{n} real numbers --- where the total sum of all numbers in the vectors equals exactly \textit{1}. Unlike a one-hot encoded vector, each number in the vector represents a probability and not a yes/no certainty},
    sort=softmax
}
\newglossaryentry{l2loss}
{
    name={L2 loss},
    description={A loss function which increases the deviation between an expected and actual value, forcing small errors to become significant. Based on the Mean of Squared Error, it is represented in this dissertation as \(l=\frac{\sum(l)^2}{2}\)},
    sort=l2loss
}
\newglossaryentry{darts_g}
{
    name={Differentiable ARchiTecture Search},
    description={An architecture search technique for learning monolithic neural architectures which also supports gradient descent}
}
\newglossaryentry{vd_g}
{
    name={Visual Dialog},
    description={An extension of \gls{vqa} which involves maintaining dialogue through multiple questions and answers instead of facing single question and answer. }
}
\newglossaryentry{vpb}
{
    name={visual priming bias},
    description={A phenomenon whereby questions asked about an image typically only mention subjects found in the image. This pattern of focusing on visible subjects is known as visual priming bias.}
}
\newglossaryentry{nas_g}
{
    name={Neural Architecture Search},
    description={A Machine Learning challenge where a model must learn the optimal architecture for a given task.}
}
\newglossaryentry{s2s_g}
{
    name={Sequence-to-Sequence},
    description={A learning strategy where an \gls{rnn_g} learns to map an input sequence to a different-length output sequence.}
}
\newglossaryentry{bilstm_g}
{
    name={Bidirectional LSTM},
    description={An \gls{rnn_g} where two \gls{lstm} cells are used to propagate information both forward (through time) and backward (recall past information) for each time step.}
}
\newglossaryentry{bpe_g}
{
    name={Byte-Pair Encoding},
    description={ A compression algorithm which merges frequent character/byte sequences into a sequence. }
}
