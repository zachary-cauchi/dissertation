\newglossaryentry{G}
{
    name=\(\mathcal{G}\),
    description={Loop-free directed graph representing the network topology},
    sort=G
}
\newglossaryentry{V}
{
    name=\(V\),
    description={The set of nodes},
    sort=V
}
\newglossaryentry{E}
{
    name=\(E\),
    description={The set of links},
    sort=E
}

\newglossaryentry{token}
{
    name=token,
    description={A word, number, symbol, or other character forming part of a sentence or array of tokens},
    sort=token
}

\newglossaryentry{corpus}
{
    name=corpus,
    description={A file containing every tokenised sentence in a dataset / series of sentences},
    sort=corpus
}
\newglossaryentry{rnn_g}
{
    name={RNN},
    description={A Recurrent Neural Network (RNN) is a class of neural network structures that
    can use its own output as input, in a cyclic manner. This allows it to process input data where order of sequence is important in deriving output, or process data of variable length},
    sort=rnn
}
\newglossaryentry{logit}
{
    name={logit},
    description={A function which represents a probability measured from 0 to 1. Mathematically, a logit is represented as \(logit(p)=log(\frac{p}{1-p})\)},
    sort=logit
}
\newglossaryentry{softmax}
{
    name={softmax},
    description={A vector of \textit{n} possible choices --- represented as \textit{n} real numbers --- where the total sum of all numbers in the vectors equals exactly \textit{1}. Unlike a one-hot encoded vector, each number in the vector represents a probability and not a yes/no certainty},
    sort=softmax
}
\newglossaryentry{l2loss}
{
    name={L2 loss},
    description={A loss function which increases the deviation between an expected and actual value, forcing small errors to become significant. Based on the Mean of Squared Error, it is represented in this dissertation as \(l=\frac{\sum(l)^2}{2}\)},
    sort=l2loss
}