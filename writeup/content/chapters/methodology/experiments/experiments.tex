\section{Experiments}
\label{sec:experiments}

Several experiments were conducted on the model to evaluate its performance on \gls{vcr}.
The experiments were designed to test the models accuracy over the three main task types (Q\rightarrow{}A, QA\rightarrow{}R, Q\rightarrow{}AR) discussed in Section~\ref{subsec:vcr_dataset} using different word embedding approaches and input encoder configurations.
Combinations of task type, \gls{bilstm} configuration in the model input unit and input token embedding types (contextual and noncontextual) were chosen as experiments to target what factors improve performance and what task types see the biggest improvement in prediction accuracy.

An additional experiment was also conducted on QA\rightarrow{}R to determine how big an influence the input answers had on the predicted rationale, and whether using previously-predicted answers as input would result in a significant drop in accuracy.
To test this, the model was first trained to predict the correct answers in Q\rightarrow{}A mode, exporting the predicted answers to a prediction file.
Then a new model was trained in QA\rightarrow{}R mode, using the predicted correct answers instead of the true correct answers.

\subsection{Setup}
\label{subsec:experiment-setup}

The model was trained on a single machine node with up to two \acrshort{gpu} nodes to use, depending on the configuration used for each experiment.
Each model was trained using a configuration file that selected the hyper-parameters used and task type to be tested.
To keep track of the model training history, model checkpoints were taken during training after every predefined number of steps.
Training was performed up to the specified step count, after which the model was evaluated using each of the recorded checkpoints.
To avoid recording the performance of an overfit model, the model performance chosen was taken from the step with the highest accuracy and not the most recent step.

\subsection{Ablations}
\label{subsec:experiment-ablations}

A set of ablation experiments were developed as part of the main experiments to determine the accuracy contribution of various components of the model.
Namely, those experiments deciding embedding types and \gls{bilstm} configuration.

The ablation tested is the use of context-aware token embeddings over context-free token embeddings, and whether a higher embedding dimensionality contributes to improved performance or not.
For this, BERT was chosen as the sentence-level context-aware embedding, retaining the same embeddings published by \citeauthor{zellers_recognition_2019} alongside the \gls{vcr} dataset and their \gls{r2c} model\cite{zellers_recognition_2019}.
To test context-free embeddings, GLOVE is used since it is the same embedding scheme used by \gls{snmn} and \gls{n2nmn} in their training and evaluation.
As an additional measure in testing context-free embeddings, Word2Vec embeddings are used which are generated using a Continuous Bag-Of-Words model\cite{mikolov_we_2013}.
Two sets of Word2Vec embeddings are generated, one with 300-dimensional vectors to match the GLOVE dimensionality and one with 768-dimensional vectors to match the BERT vectors.
This will both determine how big an effect both vector dimensionality and sentence-level context have on accuracy.

Another ablation is the testing of how the model input unit encodes the input sentences.
Seeing as the original model was designed with only a single token sequence in mind, one \gls{bilstm} encodes the sequence, but since we have more than one sequence as input, multiple \glspl{bilstm} are needed to encode each sequence.
As an ablation, another experiment is conducted where the model uses a single \gls{bilstm} to encode all three sequences.
This evaluates whether a single \gls{bilstm} would bottleneck the layout generation or improve it.
