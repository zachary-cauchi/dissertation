\chapter{Results}
\label{chp:results}

Now that the methodology has been covered, the results of the experiments described in Section~\ref{sec:experiments} will be presented.
A discussion of the results and the challenges encountered will also be given.

\section{Evaluation results}
\label{sec:evaluation-results}

The accuracy results for the experiments can be found in Table~\ref{tab:experiment-results}.
All experiments were carried out by training on the vcr-train set first, then testing against the vcr-eval set.
Each experiment model is trained for up to 75k steps with checkpoints taken at 5k step intervals.
The batch size used is 64 for Q\rightarrow{}A tasks, 16 for QA\rightarrow{}R and Q\rightarrow{}AR using BERT, and 24/32 for QA\rightarrow{}R and Q\rightarrow{}AR using Word2Vec-768 and Word2Vec-300 respectively.

\todo[inline]{Explain why different batch sizes were used (limitations of runtime).}

In every task type, the BERT embeddings model outperformed the other models with GLOVE or Word2Vec embeddings and eaching as high as 63\% in Q\rightarrow{}A tasks.
GLOVE achieved the second-best performance overall with up to 52\% Q\rightarrow{}A, but showing no learning signs in Q\rightarrow{}AR tasks with an avg. accuracy only 0.15\% higher than random guessing.
Word2Vec performend the worst across all tasks and failed to complete the full training course on QAp\rightarrow{}R and Q\rightarrow{}AR tasks.
The results suggest that the contextual embeddings generated by BERT contribute significantly to the model performance (aligning with the results found by \citeauthor{zellers_recognition_2019} where using GLOVE also resulted in worse accuracy\cite{zellers_recognition_2019}).

When performing the QAp\rightarrow{}R tasks, BERT achieved 60\% while GLOVE failed to achieve a meaningfully higher score than random guessing (26\% compared to 25\%) and Word2Vec failed to complete training.
It appears the embeddings BERT produces allow for the model to compensate for possibly embeddings from possibly-incorrect answers.
This would explain why the GLOVE model fails to produce meaningful accuracy because it does not rely on sentence-level context between sentences.


\begin{table}[]
    \begin{threeparttable}
        \begin{tabularx}{\linewidth}{r||cc|ccc|cc}
            \hline
            \multicolumn{8}{c}{Experiment results (vcr-eval)} \\ \hline
             & Q\rightarrow{}A & Q\rightarrow{}A & QA\rightarrow{}R & QA\rightarrow{}R & QAp\rightarrow{}R & Q\rightarrow{}AR & Q\rightarrow{}AR \\
            Wrd Embed. & Shr. & Sep. & Shr. & Sep. & Shr. & Shr & Sep. \\
            Rand Guess & 25\% & 25\% & 25\% & 25\% & 25\% & 6.25\% & 6.25\% \\
            Glove-300 & 52\% & 51\% & 25\% & 47\% & 26\% & 6.4\%\tnote{3} & 6.4\%\tnote{3} \\
            BERT-768 & 63\% & 63\% & 60\%\tnote{3} & 60\%\tnote{3} & 60\%\tnote{3} & 24.7\%\tnote{3} & 24\%\tnote{3} \\
            w2v-300 & 33\%\tnote{1} & 34\% & 25\% & 32\%\tnote{2}\tnote{3} & - & - & - \\
            w2v-768 & 32\%\tnote{1} & 34\% & 25\% & 32\% & - & - & - \\
            \hline
        \end{tabularx}

        \begin{tablenotes}
            \item[1] Problems with loss function resulting in partial training or lack of training.
            \item[2] Training program crashed at least once and had to be resumed from prior checkpoints.
            \item[3] Trained on multi-\gls{gpu} configuration.
        \end{tablenotes}
    \end{threeparttable}
    \captionsource(Evaluation results)
        {Evaluation results from running the model across combinations of different token embeddings, VCR task types, and layout generator configurations. \textit{Shr.} refers to models with a shared \gls{bilstm} while \textit{Sep.} refers to one \gls{bilstm} per input sentence. \label{tab:experiment-results}}
        {Original performance results obtained for this dissertation.}
\end{table}

\todo[inline]{Add discussion on why some experiments are in multi-gpu and some are single-gpu. Include table comparing one of these results.}

\section{Challenges}
\label{sec:experiment-challenges}

Several problems were encountered during training.
The most common error encountered was unstable learning which resulted in NaN loss errors or slow learning.
NaN losses were frequent when training the model using word2vec embeddings and didn't occur at all when using glove or bert.
Slow learning rates were observed during glove and word2vec training, especially with Q\rightarrow{}AR training.
Another factor in getting good predictions was batch size, which was strained by the task size and amount of data involved.
To work around the limited batch size available, training on QA\rightarrow{}R and Q\rightarrow{}AR modes was performed on a multi-gpu setup to allow for increased batch size.
This setup produced better learning rates and prediction performance compared to single-gpu training, at the cost of increased runtime due to the synchronisation overhead of keeping the two gpu models in mirrored.
