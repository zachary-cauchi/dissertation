\chapter{Results}
\label{chp:results}

Now that the methodology has been covered, the results of the experiments described in Section~\ref{sec:experiments} will be presented.
A discussion of the results and the challenges encountered will also be given.

\section{Evaluation results}
\label{sec:evaluation-results}

The accuracy results for the experiments can be found in Table~\ref{tab:experiment-results}.
All experiments were carried out by training on the vcr-train set first, then testing against the vcr-eval set.
Each experiment model is trained for up to 75k steps with checkpoints taken at 5k step intervals.
The batch size used is 64 for Q\rightarrow{}A tasks, 16 for QA\rightarrow{}R and Q\rightarrow{}AR using BERT, and 24/32 for QA\rightarrow{}R and Q\rightarrow{}AR using Word2Vec-768 and Word2Vec-300 respectively.

\begin{table}[]
    \begin{threeparttable}
        \begin{tabularx}{\linewidth}{r||cc|ccc|cc}
            \hline
            \multicolumn{8}{c}{Experiment results (vcr-eval)}                                                                                                                             \\ \hline
                       & Q\rightarrow{}A & Q\rightarrow{}A & QA\rightarrow{}R & QA\rightarrow{}R         & QAp\rightarrow{}R        & Q\rightarrow{}AR         & Q\rightarrow{}AR         \\
            Wrd Embed. & Shr.            & Sep.            & Shr.             & Sep.                     & Shr.                     & Shr                      & Sep.                     \\
            Rand Guess & 25\%            & 25\%            & 25\%             & 25\%                     & 25\%                     & 6.25\%                   & 6.25\%                   \\
            Glove-300  & 52.1\%          & 51.4\%          & \textbf{60.8\%}  & 47.2\%                   & 25.7\%\tnote{3}          & 6.4\%\tnote{3}           & 6.4\%\tnote{3}           \\
            BERT-768   & \textbf{63.2\%} & \textbf{63.8\%} & 59.5\%\tnote{3}  & \textbf{60.8\%}\tnote{3} & \textbf{60.7\%}\tnote{3} & \textbf{24.7\%}\tnote{3} & \textbf{22.2\%}\tnote{3} \\
            w2v-300    & 32.5\%\tnote{1} & 35.1\%          & 25.2\%           & 32.8\%\tnote{2}\tnote{3} & -                        & -                        & -                        \\
            w2v-768    & 32.8\%\tnote{1} & 33.9\%          & 25.1\%           & 32.3\%                   & -                        & -                        & -                        \\
            \hline
        \end{tabularx}

        \begin{tablenotes}
            \item[1] Problems with loss function resulting in partial training or lack of training.
            \item[2] Training program crashed at least once and had to be resumed from prior checkpoints.
            \item[3] Trained on multi-\gls{gpu} configuration.
        \end{tablenotes}
    \end{threeparttable}
    \captionsource(Evaluation results)
    {Evaluation results from running the model across combinations of different token embeddings, VCR task types, and layout generator configurations. \textit{Shr.} refers to models with a shared \gls{bilstm} while \textit{Sep.} refers to one \gls{bilstm} per input sentence. \label{tab:experiment-results}}
    {Original performance results obtained for this dissertation.}
\end{table}

\section{Challenges}
\label{sec:experiment-challenges}

Several problems were encountered during training.
The most common problem was unstable learning which resulted in NaN loss errors or slow learning.
NaN losses were frequent when training the model using word2vec embeddings and didn't occur at all when using GLOVE or BERT.
Slow learning rates were observed during glove and word2vec training, especially with Q\rightarrow{}AR training.
Another factor in getting good predictions was batch size, which was strained by the task size and amount of data involved.
To work around the limited batch size available, training on QA\rightarrow{}R and Q\rightarrow{}AR modes was performed on a multi-\gls{gpu} setup to allow for increased batch size.
This setup appeared to produce better learning rates and prediction performance compared to single-\gls{gpu} training in some models, at the cost of increased runtime due to the overhead of keeping the training parameters synchronised between the two \gls{gpu} models.
To examine these findings, an experiment was conducted by comparing the performance of a model trained on multiple \glspl{gpu} with increased total batch size vs single-\gls{gpu} training, the results for which can be found in Table~\ref{tab:single-vs-multi-gpu-experiment-results}.
While it appears that the model does indeed improve on the results obtained, they do not appear to be stable, with the 2-\gls{gpu}-batch24 result performing 0.1\% worse than the 1-\gls{gpu}-batch24 model.

\begin{table}[]
    \begin{threeparttable}
        \begin{tabular}{rc|c}
            \toprule
            \multicolumn{3}{c}{QA\rightarrow{}R Shr performance} \\
            \midrule
            \gls{gpu} Count & Unit Batch Size & Performance      \\
            2               & 32              & 25.9\%\tnote{1}  \\
            2               & 24              & 25.4\%           \\
            1               & 48              & 25.6\%           \\
            1               & 24              & 25.5\%           \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \item[1] A batch size of 48 was attempted, but resulted in Out-of-Memory errors on the training environment.
        \end{tablenotes}
    \end{threeparttable}
    \captionsource(Multi-GPU vs Single-GPU results)
    {Experiment results comparing differences in training performance between single-\gls{gpu} and multi-\gls{gpu} setups on QA\rightarrow{}R tasks with shared \gls{bilstm} and GLOVE embeddings. The batch size in multi-\gls{gpu} experiments is per unit \gls{gpu} and must be multiplied by the number of \glspl{gpu} to obtain the true batch size. \label{tab:single-vs-multi-gpu-experiment-results}}
    {Original performance results obtained for this dissertation.}
\end{table}

\subsection{Discussion of results}
\label{subsec:discussion_of_results}

In almost every task type, the BERT embeddings model outperformed the other models with GLOVE or Word2Vec embeddings and eaching as high as 63\% in Q\rightarrow{}A tasks.
GLOVE achieved the second-best performance overall with up to 52\% in Q\rightarrow{}A, but showing no learning signs in Q\rightarrow{}AR tasks with an avg. accuracy only 0.15\% higher than random guessing.
Word2Vec performend the worst across all tasks and failed to complete the full training course on QAp\rightarrow{}R and Q\rightarrow{}AR tasks.
The results suggest that the contextual embeddings generated by BERT contribute significantly to the model performance (aligning with the results found by \citeauthor{zellers_recognition_2019} where using GLOVE also resulted in worse accuracy\cite{zellers_recognition_2019}).

When performing the QAp\rightarrow{}R tasks, BERT achieved 60\% while GLOVE failed to achieve a meaningfully higher score than random guessing (26\% compared to 25\%) and Word2Vec failed to complete training.
It appears the BERT embeddings might allow for the model to compensate for possibly-incorrect answers, although this would need to be explored further.
If true, this may explain why the GLOVE and Word2Vec models fail to produce meaningful accuracy as it does not rely on sentence-level context between sentences.

Interestingly, most models seem likely to overfit, producing peak accuracy at evaluation checkpoints between 15k-30k iterations for Q\rightarrow{}A and QA\rightarrow{}R tasks.
This behaviour is most apparent in the BERT models where prediction accuracy peaks at around 20k iterations before regressing.
The Q\rightarrow{}AR task was less likely overfit, with the model peaking at the end of training, suggesting more training iterations are needed.
It might be suitable to explore different training strategies in the future which would help prevent overfitting, such as the training strategy used by \gls{dpnmn} as discussed in Chapter~\ref{subsec:dual_path_neural_module_network}.


\subsection{Qualitative analysis against other VCR models}
\label{subsec:qualitative_analysis_against_other_vcr_models}

The top results from Table~\ref{tab:experiment-results} are compared against the other \gls{vcr} models in Table~\ref{tab:snmn_vs_other_vcr_models}.
As expected, the model does not outperform the \gls{vcr} models, being almost 40\% less accurate in Q\rightarrow{}AR tasks when compared to MERLOT-RESERVE.
Q\rightarrow{}A and QA\rightarrow{}R however produced comparable results to the \gls{r2c} model, only being 6.5\% worse at most.
Given the large difference in accuracy between BERT and GLOVE, a large factor in the performance similarity might be attributed to BERT.
It seems as though MERLOT-RESERVE might be making a large improvement thanks to the increased generalisability of both the model owing to its training, and for its subword-based embeddings using \gls{bpe} tables.
This would align with the growing number of generalised models such as \gls{mmn}, \gls{lnmn}, and now MERLOT-RESERVE.
Another possible contributing factor is the training approach; whereas this \gls{snmn} model trained solely on the \gls{vcr}-train set, MERLOT-RESERVE pretrained on a much larger dataset combining different data sources (image, text, and audio) in various combinations, and then fine-tuned onto \gls{vcr} for testing.

Besides the embeddings themselves, there may also be the problem of subject inferrence.
Currently, the model preprocesses the dataset before training on it such that unique instances of an object are replaced by the generic object name and so sentences can often become saturated with subjects (eg: a sentence like 'Why did [1] and [2] steal [3]'s bike' would become 'Why did person and person steal person's bike').
If there were a way for the model to better distinguish each object reference (such as the visual coreference resolution approach used by \gls{nmnvd}\cite{cho_visual_2021}), the model might perform better in the QA\rightarrow{}R and Q\rightarrow{}AR tasks.

\begin{table}[]
    \begin{tabular}{l|ccc}
        \toprule
        \multicolumn{4}{c}{Results comparison}                                     \\
        \midrule
        Model              & Q\rightarrow{}A & QA\rightarrow{}R & Q\rightarrow{}AR \\
        \gls{vcr} (val)    & 63.8\%          & 67.2\%           & 43.1\%           \\
        \gls{vcr} (test)   & 65.1\%          & 67.3\%           & 44.0\%           \\
        MERLOT-RESERVE (L) & \textbf{84.0\%} & \textbf{84.9\%}  & \textbf{72.0\%}  \\
        \midrule
        \gls{snmn}         & 63.8\%          & 60.8\%           & 24.7\%           \\
        \bottomrule
    \end{tabular}
    \captionsource(Experiment results vs other VCR models)
        {Experiment results of the \gls{snmn} model compared to the other \gls{snmn} models. The results chosen were the highest-accuracy models from the previously-discussed experiments. \label{tab:snmn_vs_other_vcr_models}}
        {\gls{r2c} results: \citeauthor{zellers_recognition_2019}\cite{zellers_recognition_2019}, MERLOT-RESERVE results: \citeauthor{zellers_merlot_2022}\cite{zellers_merlot_2022}, \gls{snmn} results: Original performance results obtained for this dissertion.}
\end{table}
